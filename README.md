# Nanoformers

A minimal playground for building and training transformer models from scratch.
It covers self-supervised, supervised, and reinforcement learning training loops, along with a tiny transformer architecture for research and experimentation.

## 🔍 Objectives

* Implement tiny transformer architectures from scratch
* Build training loops:
    * Self-supervised learning 
        * [ ] Causal Language Modeling
    * Supervised learning
        * [ ] Instruction Fine-tuning
        * [ ] Direct Preference Optimization
    * Reinforcement learning
        * [ ] Proximal Policy Optimization
        * [ ] Group Relative Policy Optimization

## 📰 News

## 🚀 Models Trained 
